{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:09.005059Z",
     "iopub.status.busy": "2024-12-06T12:34:09.004361Z",
     "iopub.status.idle": "2024-12-06T12:34:09.009581Z",
     "shell.execute_reply": "2024-12-06T12:34:09.008692Z",
     "shell.execute_reply.started": "2024-12-06T12:34:09.005028Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import collections.abc\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "import torch.utils.checkpoint\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:09.769645Z",
     "iopub.status.busy": "2024-12-06T12:34:09.769314Z",
     "iopub.status.idle": "2024-12-06T12:34:09.775781Z",
     "shell.execute_reply": "2024-12-06T12:34:09.774825Z",
     "shell.execute_reply.started": "2024-12-06T12:34:09.769613Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchConfig:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=8,\n",
    "        intermediate_size=1024,\n",
    "        hidden_dropout_prob=0.02,\n",
    "        image_size=32,\n",
    "        patch_size=4,\n",
    "        num_channels=3,\n",
    "        num_blocks=6\n",
    "    ):\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_blocks=num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:10.310773Z",
     "iopub.status.busy": "2024-12-06T12:34:10.310000Z",
     "iopub.status.idle": "2024-12-06T12:34:10.316584Z",
     "shell.execute_reply": "2024-12-06T12:34:10.315898Z",
     "shell.execute_reply.started": "2024-12-06T12:34:10.310743Z"
    }
   },
   "outputs": [],
   "source": [
    "x=PatchConfig()\n",
    "x.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:11.512923Z",
     "iopub.status.busy": "2024-12-06T12:34:11.512575Z",
     "iopub.status.idle": "2024-12-06T12:34:11.526819Z",
     "shell.execute_reply": "2024-12-06T12:34:11.525937Z",
     "shell.execute_reply.started": "2024-12-06T12:34:11.512878Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    '''\n",
    "    Construct the position and patch embeddings.\n",
    "    Args:\n",
    "        config: Configuration object containing model parameters.\n",
    "        use_mask_token (bool, optional): Whether to use a mask token. Defaults to False.\n",
    "    Attributes:\n",
    "        config: Configuration object containing model parameters.\n",
    "        cls_token (nn.Parameter): Learnable [CLS] token.\n",
    "        patch_embeddings (PatchPatchEmbeddings): Patch embeddings module.\n",
    "        position_embeddings (nn.Parameter): Learnable position embeddings.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        patch_size (int): Size of each patch.\n",
    "        hidden_size (int): Size of the hidden layer.\n",
    "    Methods:\n",
    "        pos_encoding(embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "            Computes the positional encoding for the given embeddings.\n",
    "        forward(pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.BoolTensor] = None, pos_encoding: bool = True) -> torch.Tensor:\n",
    "            Forward pass to compute the embeddings for the input pixel values.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, config, use_mask_token: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.config.hidden_size))\n",
    "        self.patch_embeddings = PatchPatchEmbeddings(self.config)\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, self.config.hidden_size))\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.patch_size = self.config.patch_size\n",
    "        self.hidden_size = self.config.hidden_size\n",
    "\n",
    "    def pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "   \n",
    "        num_patches = embeddings.shape[1] - 1\n",
    "        num_positions = self.position_embeddings.shape[1] - 1\n",
    "\n",
    "        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n",
    "        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n",
    "            return self.position_embeddings\n",
    "\n",
    "        class_pos_embed = self.position_embeddings[:, :1]\n",
    "        patch_pos_embed = self.position_embeddings[:, 1:]\n",
    "\n",
    "        dim = embeddings.shape[-1]\n",
    "\n",
    "        new_height = height // self.patch_size\n",
    "        new_width = width // self.patch_size\n",
    "\n",
    "        sqrt_num_positions =  int(math.sqrt(num_positions)) # if num_positions is a tensor\n",
    "\n",
    "        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n",
    "\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed,\n",
    "            size=(new_height, new_width),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
    "        pos_encoding: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values, pos_encoding=pos_encoding)\n",
    "\n",
    "\n",
    "        # add the [CLS] token to the embedded patch tokens\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings),\n",
    "                               dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        if pos_encoding:\n",
    "            embeddings = embeddings + self.pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings  #(batch_size,num_patches+1,hidden_size)\n",
    "\n",
    "class PatchPatchEmbeddings(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        image_size, patch_size = self.config.image_size, self.config.patch_size\n",
    "        num_channels, hidden_size = self.config.num_channels, self.config.hidden_size\n",
    "\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, pos_encoding: bool = True) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "        if not pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2) #(batch_size,num_patches,hidden_size)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:12.121514Z",
     "iopub.status.busy": "2024-12-06T12:34:12.120714Z",
     "iopub.status.idle": "2024-12-06T12:34:12.127829Z",
     "shell.execute_reply": "2024-12-06T12:34:12.127120Z",
     "shell.execute_reply.started": "2024-12-06T12:34:12.121481Z"
    }
   },
   "outputs": [],
   "source": [
    "x = PatchEmbeddings(PatchConfig())\n",
    "x.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:12.791630Z",
     "iopub.status.busy": "2024-12-06T12:34:12.791303Z",
     "iopub.status.idle": "2024-12-06T12:34:12.799523Z",
     "shell.execute_reply": "2024-12-06T12:34:12.798589Z",
     "shell.execute_reply.started": "2024-12-06T12:34:12.791593Z"
    }
   },
   "outputs": [],
   "source": [
    "class HolloPatchBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, expansion_dim, in_channels=3):\n",
    "        \"\"\"\n",
    "        Initializes the HolloPatchBlock module.\n",
    "        Args:\n",
    "            embed_dim (int): The dimension of the embedding.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            expansion_dim (int): The dimension to which the feed-forward network expands.\n",
    "            in_channels (int, optional): The number of input channels. Default is 3.\n",
    "        Attributes:\n",
    "            norm1 (nn.LayerNorm): Layer normalization for the input embeddings.\n",
    "            mha (nn.MultiheadAttention): Multi-head attention mechanism.\n",
    "            norm2 (nn.LayerNorm): Layer normalization after the attention mechanism.\n",
    "            ffn (nn.Sequential): Feed-forward network consisting of two linear layers with a GELU activation in between.\n",
    "            embed_dim (int): The dimension of the embedding.\n",
    "            batch_norm1 (nn.BatchNorm2d): Batch normalization for the input channels.\n",
    "            t2i_conv1 (nn.Conv2d): Convolutional layer transforming input channels to 9 times the input channels.\n",
    "            batch_norm2 (nn.BatchNorm2d): Batch normalization for the transformed channels.\n",
    "            t2i_conv2 (nn.Conv2d): Convolutional layer transforming back to the original input channels.\n",
    "            conv_trans1 (nn.ConvTranspose2d): Transposed convolutional layer transforming embeddings back to input channels.\n",
    "        \"\"\"\n",
    "        super(HolloPatchBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expansion_dim, embed_dim),\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "        self.batch_norm1=nn.BatchNorm2d(in_channels)\n",
    "        self.t2i_conv1 = nn.Conv2d(in_channels, in_channels*9, kernel_size=3, padding=1)\n",
    "        self.batch_norm2=nn.BatchNorm2d(in_channels*9)\n",
    "        self.t2i_conv2 = nn.Conv2d(in_channels*9, in_channels, kernel_size=3, padding=1)\n",
    "        self.conv_trans1 = nn.ConvTranspose2d(embed_dim, in_channels, kernel_size=4, stride=4)\n",
    "\n",
    "    def forward(self, patch_embeddings, image_embeddings):\n",
    "        # Multi-head Attention\n",
    "        patch_embeddings = patch_embeddings + self.mha(self.norm1(patch_embeddings), patch_embeddings, patch_embeddings)[0]\n",
    "        # Feed Forward Network\n",
    "        patch_embeddings = patch_embeddings + self.ffn(self.norm2(patch_embeddings))\n",
    "        patch_1 = patch_embeddings[:, 1:, :].permute(0,2,1).reshape(-1, self.embed_dim, 8, 8)\n",
    "        patch_image = self.conv_trans1(patch_1)\n",
    "        \n",
    "        # T2I Block for combining patch and image embeddings\n",
    "        t2i_out = F.relu(self.t2i_conv1(self.batch_norm1(image_embeddings)))\n",
    "        t2i_out = F.relu(self.t2i_conv2(self.batch_norm2(t2i_out)))\n",
    "\n",
    "        combined_embeddings = patch_image + t2i_out  # Combining along the channel dimension\n",
    "        return patch_embeddings,combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:13.415647Z",
     "iopub.status.busy": "2024-12-06T12:34:13.415307Z",
     "iopub.status.idle": "2024-12-06T12:34:13.424653Z",
     "shell.execute_reply": "2024-12-06T12:34:13.423783Z",
     "shell.execute_reply.started": "2024-12-06T12:34:13.415618Z"
    }
   },
   "outputs": [],
   "source": [
    "HolloPatch=HolloPatchBlock(128, 8, 512)\n",
    "HolloPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:13.771932Z",
     "iopub.status.busy": "2024-12-06T12:34:13.771599Z",
     "iopub.status.idle": "2024-12-06T12:34:13.778115Z",
     "shell.execute_reply": "2024-12-06T12:34:13.777160Z",
     "shell.execute_reply.started": "2024-12-06T12:34:13.771904Z"
    }
   },
   "outputs": [],
   "source": [
    "class SignatureExtractor(nn.Module):\n",
    "    def __init__(self, config, in_channels=3):\n",
    "        \"\"\"\n",
    "        Initializes the SignatureExtractor class.\n",
    "        Args:\n",
    "            config (object): Configuration object containing model parameters.\n",
    "            in_channels (int, optional): Number of input channels. Default is 3.\n",
    "        Attributes:\n",
    "            patch_embed (PatchEmbeddings): Embedding layer for patches.\n",
    "            embed_dim (int): Dimension of the embeddings.\n",
    "            num_attention_heads (int): Number of attention heads.\n",
    "            expansion_dim (int): Dimension of the intermediate expansion layer.\n",
    "            HolloPatch_blocks (nn.ModuleList): List of HolloPatchBlock modules.\n",
    "            conv (nn.Conv2d): Convolutional layer for rectified image.\n",
    "        \"\"\"\n",
    "        super(SignatureExtractor, self).__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        self.patch_embed = PatchEmbeddings(config) #(batch,num_patch+1,hidden_size(embed_dim))\n",
    "        self.embed_dim=config.hidden_size\n",
    "        self.num_attention_heads= config.num_attention_heads\n",
    "        self.expansion_dim =  config.intermediate_size\n",
    "        self.HolloPatch_blocks = nn.ModuleList([\n",
    "            HolloPatchBlock(self.embed_dim, self.num_attention_heads, self.expansion_dim) for _ in range(config.num_blocks)\n",
    "        ])\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)  # Rectified Image\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        patch_embeddings = self.patch_embed(x)  #(batch,num_patch+1,hidden_size(embed_dim))\n",
    "        image_embeddings = x\n",
    "        \n",
    "        for HolloPatch_block in self.HolloPatch_blocks:\n",
    "            patch_embeddings, image_embeddings = HolloPatch_block(patch_embeddings, image_embeddings)\n",
    "            \n",
    "        rectified_image = self.conv(image_embeddings)\n",
    "        \n",
    "        return rectified_image, x-rectified_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:14.303923Z",
     "iopub.status.busy": "2024-12-06T12:34:14.303591Z",
     "iopub.status.idle": "2024-12-06T12:34:14.332691Z",
     "shell.execute_reply": "2024-12-06T12:34:14.331882Z",
     "shell.execute_reply.started": "2024-12-06T12:34:14.303892Z"
    }
   },
   "outputs": [],
   "source": [
    "x = SignatureExtractor(PatchConfig())\n",
    "x.patch_embed.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:14.772926Z",
     "iopub.status.busy": "2024-12-06T12:34:14.772597Z",
     "iopub.status.idle": "2024-12-06T12:34:14.779396Z",
     "shell.execute_reply": "2024-12-06T12:34:14.778555Z",
     "shell.execute_reply.started": "2024-12-06T12:34:14.772898Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelPipeline(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model pipeline for image processing and classification.\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary for the SignatureExtractor.\n",
    "        in_channels (int, optional): Number of input channels for the convolutional layer. Default is 3.\n",
    "        num_classes (int, optional): Number of output classes for the classifier. Default is 2.\n",
    "    Attributes:\n",
    "        signature_extractor (SignatureExtractor): Module to extract signatures from input images.\n",
    "        conv (nn.Conv2d): Convolutional layer to process input images.\n",
    "        batch_norm (nn.BatchNorm2d): Batch normalization layer for the convolutional output.\n",
    "        fc1 (nn.Linear): Fully connected layer for classification.\n",
    "        attack_classifier (nn.Softmax): Softmax layer to produce class probabilities.\n",
    "        flatten (nn.Flatten): Flatten layer to reshape the tensor for the fully connected layer.\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Forward pass of the model.\n",
    "            Args:\n",
    "                x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width).\n",
    "            Returns:\n",
    "                tuple: A tuple containing:\n",
    "                    - class_probs (torch.Tensor): Class probabilities of shape (batch_size, num_classes).\n",
    "                    - signature (torch.Tensor): Extracted signature from the input image.\n",
    "                    - rectified_image (torch.Tensor): Rectified image from the SignatureExtractor.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config,in_channels=3, num_classes=2):\n",
    "        super(ModelPipeline, self).__init__()\n",
    "        self.signature_extractor = SignatureExtractor(config)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels*5, kernel_size=3, padding=1)\n",
    "        self.batch_norm=nn.BatchNorm2d(in_channels*5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_channels*5*2*32*32, num_classes)\n",
    "        self.attack_classifier = nn.Softmax(dim=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        rectified_image,signature = self.signature_extractor(x)\n",
    "        image = self.batch_norm(self.conv(x))\n",
    "        sign = self.batch_norm(self.conv(signature))\n",
    "        concatenated_image = torch.cat([image, sign], dim=1)\n",
    "        fc_output = self.fc1(self.flatten(concatenated_image))\n",
    "        class_probs = self.attack_classifier(fc_output)\n",
    "\n",
    "        return class_probs,signature,rectified_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:15.378270Z",
     "iopub.status.busy": "2024-12-06T12:34:15.377967Z",
     "iopub.status.idle": "2024-12-06T12:34:15.412019Z",
     "shell.execute_reply": "2024-12-06T12:34:15.411218Z",
     "shell.execute_reply.started": "2024-12-06T12:34:15.378243Z"
    }
   },
   "outputs": [],
   "source": [
    "x = ModelPipeline(PatchConfig())\n",
    "x.signature_extractor.patch_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:15.874875Z",
     "iopub.status.busy": "2024-12-06T12:34:15.874337Z",
     "iopub.status.idle": "2024-12-06T12:34:16.231381Z",
     "shell.execute_reply": "2024-12-06T12:34:16.230194Z",
     "shell.execute_reply.started": "2024-12-06T12:34:15.874827Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "aid_dataset = datasets.ImageFolder(root=\".corrected_dataset/train\", transform=transform)\n",
    "\n",
    "# Define train-val split ratio\n",
    "train_ratio = 0.8\n",
    "val_ratio = 1 - train_ratio\n",
    "\n",
    "# Calculate dataset sizes\n",
    "total_size = len(aid_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# Split the dataset into train and val subsets\n",
    "train_dataset, val_dataset = random_split(aid_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Create DataLoaders for train and val subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Batch size can be adjusted\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:16.425408Z",
     "iopub.status.busy": "2024-12-06T12:34:16.425118Z",
     "iopub.status.idle": "2024-12-06T12:34:16.455054Z",
     "shell.execute_reply": "2024-12-06T12:34:16.454308Z",
     "shell.execute_reply.started": "2024-12-06T12:34:16.425380Z"
    }
   },
   "outputs": [],
   "source": [
    "let=PatchConfig()\n",
    "model = ModelPipeline(let)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:16.991928Z",
     "iopub.status.busy": "2024-12-06T12:34:16.991554Z",
     "iopub.status.idle": "2024-12-06T12:34:16.997229Z",
     "shell.execute_reply": "2024-12-06T12:34:16.996357Z",
     "shell.execute_reply.started": "2024-12-06T12:34:16.991893Z"
    }
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:34:20.001426Z",
     "iopub.status.busy": "2024-12-06T12:34:20.001026Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "in_channels = 3\n",
    "num_classes = 2  # Assuming binary classification for adversarial attacks\n",
    "\n",
    "# Initialize your model\n",
    "model = ModelPipeline(PatchConfig())\n",
    "\n",
    "# Wrap the model with DataParallel\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Move the model to GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "classification_loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Initialize the scheduler to reduce learning rate on plateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-15)\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "min_delta = 0.001  # Minimum change to consider as improvement\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for image, labels in train_loader:\n",
    "        labels = F.one_hot(labels, num_classes=num_classes).float()  # Shape: (batch_size, num_classes)\n",
    "        image, labels = image.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        classification_output, signature, rectified_image = model(image)\n",
    "        classification_loss = classification_loss_fn(classification_output, labels)\n",
    "        classification_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += classification_loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        predicted_labels = torch.argmax(classification_output, dim=1)  # Predicted class\n",
    "        true_labels = torch.argmax(labels, dim=1)  # True class\n",
    "        train_correct += (predicted_labels == true_labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in val_loader:\n",
    "            labels = F.one_hot(labels, num_classes=num_classes).float()\n",
    "            embeddings, labels = embeddings.to('cuda'), labels.to('cuda')\n",
    "            outputs, signature, rectified_image = model(embeddings)\n",
    "            loss = classification_loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)  # Predicted class\n",
    "            true_labels = torch.argmax(labels, dim=1)  # True class\n",
    "            val_correct += (predicted_labels == true_labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Training Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "          f\"Training Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Scheduler step based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'model{epoch+1}.pth')\n",
    "\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # Reset counter if validation loss improves\n",
    "        torch.save(model.state_dict(), 'final_best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load the best model after training if needed\n",
    "model.load_state_dict(torch.load('final_best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T13:56:20.868526Z",
     "iopub.status.busy": "2024-11-27T13:56:20.867919Z",
     "iopub.status.idle": "2024-11-27T13:56:20.910086Z",
     "shell.execute_reply": "2024-11-27T13:56:20.909266Z",
     "shell.execute_reply.started": "2024-11-27T13:56:20.868484Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"entire_model.pth\")\n",
    "print(\"Entire model saved as entire_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-27T11:07:35.455862Z",
     "iopub.status.idle": "2024-11-27T11:07:35.456149Z",
     "shell.execute_reply": "2024-11-27T11:07:35.456022Z",
     "shell.execute_reply.started": "2024-11-27T11:07:35.456008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your model instance is called `model`\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "print(\"Model weights saved as model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-27T11:07:35.457426Z",
     "iopub.status.idle": "2024-11-27T11:07:35.457712Z",
     "shell.execute_reply": "2024-11-27T11:07:35.457593Z",
     "shell.execute_reply.started": "2024-11-27T11:07:35.457579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming the same architecture is defined or imported\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "model.eval()  # Set to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-27T11:07:35.458586Z",
     "iopub.status.idle": "2024-11-27T11:07:35.458890Z",
     "shell.execute_reply": "2024-11-27T11:07:35.458755Z",
     "shell.execute_reply.started": "2024-11-27T11:07:35.458740Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"entire_model.pth\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "test_dataset = datasets.ImageFolder(root=\".corrected_dataset/test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('final_best_model.pth'))\n",
    "\n",
    "# Test phase\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "# Store predictions and true labels for evaluation\n",
    "all_predicted_labels = []\n",
    "all_true_labels = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        labels = F.one_hot(labels, num_classes=num_classes).float()\n",
    "        embeddings, labels = embeddings.to('cuda'), labels.to('cuda')\n",
    "        outputs, signature, rectified_image = model(embeddings)\n",
    "        loss = classification_loss_fn(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)  # Predicted class\n",
    "        true_labels = torch.argmax(labels, dim=1)  # True class\n",
    "        test_correct += (predicted_labels == true_labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "        # Store predictions and true labels for metrics\n",
    "        all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "        all_true_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Calculate precision, recall, F1 score, and confusion matrix\n",
    "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3041726,
     "sourceId": 5256696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6219736,
     "sourceId": 10087498,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
