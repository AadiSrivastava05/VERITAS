{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy.signal import convolve2d\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import shutil\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 1\n",
    "num_classes = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required data appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping custom_dataset.zip\n",
    "#The unzipping results in 2 datasets - custom_dataset and test_dataset - being created in the working directory\n",
    "with zipfile.ZipFile(\"custom_dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall('custom_dataset')\n",
    "\n",
    "# Unzipping tes-final.zip\n",
    "with zipfile.ZipFile(\"test_dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall('test_dataset')\n",
    "\n",
    "# Configuration\n",
    "train_dir = \"/kaggle/input/custom_dataset/train\"\n",
    "test_dir = \"/kaggle/input/custom_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove .ipynb_checkpoints folder from the dataset\n",
    "def remove_ipynb_checkpoints(dataset_dir):\n",
    "    for root, dirs, files in os.walk(dataset_dir, topdown=False):\n",
    "        # Check if .ipynb_checkpoints is in the directories list\n",
    "        if '.ipynb_checkpoints' in dirs:\n",
    "            # Construct the full path to the .ipynb_checkpoints directory\n",
    "            checkpoint_dir = os.path.join(root, '.ipynb_checkpoints')\n",
    "            # Remove the .ipynb_checkpoints directory and all its contents\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "# Paths to the train and test directories\n",
    "train_dir = 'custom_dataset/train'\n",
    "test_dir = 'custom_dataset/test'\n",
    "\n",
    "# Remove .ipynb_checkpoints from both train and test directories\n",
    "remove_ipynb_checkpoints(train_dir)\n",
    "remove_ipynb_checkpoints(test_dir)\n",
    "remove_ipynb_checkpoints('test_dataset/test-interiit/perturbed_images_32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuctions to compute the FFT and LBP of an input image\n",
    "def compute_fft_magnitude(image):\n",
    "    \"\"\"\n",
    "    Computes the FFT magnitude spectrum of an input image.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor or np.ndarray): Input image (RGB).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Normalized magnitude spectrum of the FFT.\n",
    "    \"\"\"\n",
    "    # Check if input is a PyTorch tensor\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)  # Convert to NumPy\n",
    "    # Perform FFT computation\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    f = np.fft.fft2(gray)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-10)  # Avoid log(0)\n",
    "    return cv2.normalize(magnitude_spectrum, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "def compute_lbp(image):\n",
    "    \"\"\"\n",
    "    Computes the Local Binary Pattern (LBP) of an input image.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor or np.ndarray): Input image (RGB).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: LBP of the image, resized to 32x32.\n",
    "    \"\"\"\n",
    "    # Check if input is a PyTorch tensor\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)  # Convert to NumPy\n",
    "    # Perform LBP computation\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    return cv2.resize(lbp, (32, 32))\n",
    "\n",
    "#Dataset Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Match DEFL input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFL architecture development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the 8 base filters (example 3x3 filters, customize as needed)\n",
    "base_filters = [\n",
    "    np.array([[0, 0, 0], [0, -1, 1], [0, 0, 0]]),  \n",
    "    np.array([[0, 0, 0], [1, -1, 0], [0, 0, 0]]),  \n",
    "    np.array([[0, 1, 0], [0, -1, 0], [0, 0, 0]]), \n",
    "    np.array([[0, 0, 0], [0, -1, 0], [0, 1, 0]]),  \n",
    "    np.array([[0, 0, 1], [0, -1, 0], [0, 0, 0]]), \n",
    "    np.array([[0, 0, 0], [0, -1, 0], [1, 0, 0]]), \n",
    "    np.array([[1, 0, 0], [0, -1, 0], [0, 0, 0]]), \n",
    "    np.array([[0, 0, 0], [0, -1, 0], [0, 0, 1]]) \n",
    "]\n",
    "\n",
    "# List to store the composite filters\n",
    "composite_filters = []\n",
    "# Generate composite filters for n = 1 to 7\n",
    "for n in range(1, 8):\n",
    "    # Get all combinations of n base filters\n",
    "    for combo in combinations(base_filters, n):\n",
    "        # Start with the first filter in the combination\n",
    "        composite_filter = combo[0]\n",
    "        # Convolve with the remaining filters in the combination\n",
    "        for next_filter in combo[1:]:\n",
    "            composite_filter = convolve2d(composite_filter, next_filter, mode='same')\n",
    "        # Add the resulting composite filter to the list\n",
    "        composite_filters.append(composite_filter)\n",
    "# Duplicating and adding two random filters to the composite filters to make it 256\n",
    "composite_filters.append(composite_filters[0])\n",
    "composite_filters.append(composite_filters[1])\n",
    "# Shuffling the contents of my list\n",
    "composite_filters = random.sample(composite_filters, len(composite_filters))\n",
    "\n",
    "\n",
    "class DirectionalConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A block that applies a set of directional convolutions to the input image.\n",
    "\n",
    "    This block uses a set of composite directional filters, applied through \n",
    "    multiple convolutional layers, to extract directional features from the input image.\n",
    "    \n",
    "    Args:\n",
    "        composite_filters (list of numpy.ndarray): List of filter weights for the directional convolutions.\n",
    "        in_channels (int): Number of input channels of the image (e.g., 3 for RGB images).\n",
    "\n",
    "    Layers:\n",
    "        - `conv_layers`: A list of 64 convolutional layers, each applying a directional filter to the input image.\n",
    "    \n",
    "    The input is passed through each of the 64 convolutional layers, and the results are concatenated \n",
    "    along the channel dimension to produce the output feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, composite_filters, in_channels):\n",
    "        super(DirectionalConvolutionalBlock, self).__init__()\n",
    "        self.filters = composite_filters\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=3, padding=1, bias=False),\n",
    "                #nn.BatchNorm2d(3)  # Add BatchNorm\n",
    "            )\n",
    "            for _ in range(64)\n",
    "        ])\n",
    "        for i, filter_weights in enumerate(self.filters):\n",
    "            weight = torch.tensor(filter_weights, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            self.conv_layers[i][0].weight.data = weight.repeat(1, in_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the directional convolution filters to the input image.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Concatenated output tensor after applying all directional convolutions.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        spatial_size = x.size()[2:]  # Get the spatial dimensions (H, W)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            out = F.relu(conv_layer(x))  # ReLU after batch norm\n",
    "            # if out.size()[2:] != spatial_size:\n",
    "            #     out = F.interpolate(out, size=spatial_size, mode='bilinear', align_corners=False)\n",
    "            outputs.append(out)\n",
    "        return torch.cat(outputs, dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "\n",
    "class StandardConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A block that applies a standard set of convolutions to the input image.\n",
    "\n",
    "    This block uses 64 convolutional layers to extract features from the input image using standard \n",
    "    convolutions, each with a kernel size of 3 and padding of 1. The outputs of these convolutions \n",
    "    are concatenated along the channel dimension to form the output.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels of the image (e.g., 3 for RGB images).\n",
    "\n",
    "    Layers:\n",
    "        - `conv_layers`: A list of 64 convolutional layers, each applying standard convolution to the input image.\n",
    "\n",
    "    The input is passed through each of the 64 convolutional layers, and the results are concatenated \n",
    "    along the channel dimension to produce the output feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(StandardConvolutionalBlock, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=3, padding=1, bias=False),\n",
    "                #nn.BatchNorm2d(1)  # Add BatchNorm\n",
    "            )\n",
    "            for _ in range(64)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies standard convolution filters to the input image.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Concatenated output tensor after applying all convolutions.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for conv_layer in self.conv_layers:\n",
    "            out = F.relu(conv_layer(x))  # ReLU after batch norm\n",
    "            outputs.append(out)\n",
    "        return torch.cat(outputs, dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "\n",
    "class DEFL(nn.Module):\n",
    "    \"\"\"\n",
    "    DEFL (Directional and Standard Feature Learning) model for feature extraction.\n",
    "\n",
    "    The DEFL model combines two types of feature extraction blocks:\n",
    "    1. **Directional Convolutional Blocks (DCBs)**: Apply directional filters to the input image to capture directional features.\n",
    "    2. **Standard Convolutional Blocks (SCBs)**: Apply standard convolutions to extract general features.\n",
    "\n",
    "    The model is structured into multiple levels, where each level contains both a Directional and Standard Convolutional Block.\n",
    "\n",
    "    Args:\n",
    "        composite_filters (list of list of numpy.ndarray): List of directional filters used in the DCBs. \n",
    "        The list is divided into 4 sets of 64 filters each for each level.\n",
    "\n",
    "    Layers:\n",
    "        - `levels`: A list of 4 levels, where each level contains both a DCB and a SCB.\n",
    "        \n",
    "    The input is passed through each level, where the output of each block (DCB and SCB) is concatenated \n",
    "    along the channel dimension and passed to the next level. The concatenation of outputs from the last level is returned.\n",
    "    \"\"\"\n",
    "    def __init__(self, composite_filters):\n",
    "        super(DEFL, self).__init__()\n",
    "        self.levels = nn.ModuleList()\n",
    "        in_channels = 3  # Initial input has 3 channels\n",
    "        for i in range(4):\n",
    "            dcb_filters = composite_filters[i * 64:(i + 1) * 64]  # Each level gets 64 filters\n",
    "            self.levels.append(nn.ModuleDict({\n",
    "                \"dcb\": DirectionalConvolutionalBlock(dcb_filters, in_channels),\n",
    "                \"scb\": StandardConvolutionalBlock(in_channels)\n",
    "            }))\n",
    "            in_channels = 128  # After concatenating DCB and SCB outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the DEFL model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through all levels, containing concatenated \n",
    "                          feature maps from DCBs and SCBs.\n",
    "        \"\"\"\n",
    "        for level in self.levels:\n",
    "            dcb_out = level[\"dcb\"](x)  # Pass through Directional Convolutional Block\n",
    "            scb_out = level[\"scb\"](x)  # Pass through Standard Convolutional Block\n",
    "            x = torch.cat([dcb_out, scb_out], dim=1)  # Combine outputs\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model instantiation and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pretrained ViT and Access CLS Token\n",
    "class CombinedViTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Vision Transformer (ViT) model for binary classification with custom classifier head.\n",
    "\n",
    "    This model loads a pre-trained Vision Transformer (ViT) model and removes its \n",
    "    classification head. A new linear classifier is then applied to the output of \n",
    "    the CLS token (embedding) for binary classification tasks.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): Name of the pre-trained ViT model (default: 'vit_tiny_patch16_224').\n",
    "        embedding_dim (int): Dimensionality of the ViT embedding (default: 192).\n",
    "        num_classes (int): Number of output classes (default: 2, for binary classification).\n",
    "        in_channels (int): Number of input channels (default: 130).\n",
    "\n",
    "    Layers:\n",
    "        - `base_model`: Pre-trained Vision Transformer model without the classification head.\n",
    "        - `classifier`: A custom linear classifier for binary classification.\n",
    "\n",
    "    Forward Pass:\n",
    "        The input image is passed through the pre-trained ViT model to extract the CLS token \n",
    "        (embedding), which is then passed through the new classifier to obtain the logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model_name='vit_tiny_patch16_224', embedding_dim=192, num_classes=2, in_channels = 130):\n",
    "        super(CombinedViTModel, self).__init__()\n",
    "        # Load pre-trained ViT model\n",
    "        self.base_model = timm.create_model(base_model_name, pretrained=True, img_size = 32, in_chans = 130)\n",
    "        self.base_model.head = nn.Identity()  # Nullify the classification head\n",
    "        \n",
    "        # New classification head\n",
    "        self.classifier = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (batch_size, in_channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            tuple: \n",
    "                - embeddings (torch.Tensor): Extracted CLS token (embedding) from the model.\n",
    "                - logits (torch.Tensor): Output logits for binary classification.\n",
    "        \"\"\"\n",
    "        # Extract embeddings (CLS token) from the base model\n",
    "        embeddings = self.base_model.forward_features(x)[:, 0]  # CLS token is the first token\n",
    "        logits = self.classifier(embeddings)  # Binary classification logits\n",
    "        return embeddings, logits\n",
    "\n",
    "#Initialize a variable to observe the value of the 2 losses in the first run of the model, and not again\n",
    "count1 = 0\n",
    "\n",
    "# Define the Combined Loss Function\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss function with Binary Cross-Entropy (BCE) Loss and Contrastive Loss.\n",
    "\n",
    "    This loss function computes two components:\n",
    "    1. Binary Cross-Entropy loss (BCE) for binary classification.\n",
    "    2. Contrastive loss to learn feature similarities between positive and negative pairs.\n",
    "    The total loss is the weighted sum of BCE loss and contrastive loss.\n",
    "\n",
    "    Args:\n",
    "        margin (float): Margin for contrastive loss (default: 1.0).\n",
    "        temperature (float): Temperature for similarity scaling in contrastive loss (default: 0.5).\n",
    "        lambda_contrastive (float): Weight for the contrastive loss component (default: 0.01).\n",
    "\n",
    "    Layers:\n",
    "        - `bce_loss`: Binary Cross-Entropy loss function for binary classification.\n",
    "\n",
    "    Forward Pass:\n",
    "        The BCE loss is computed from the logits and labels. The contrastive loss is computed \n",
    "        from the embeddings and labels, and the two losses are combined with the weighting factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0, temperature=0.5, lambda_contrastive=0.01):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        self.lambda_contrastive = lambda_contrastive\n",
    "\n",
    "    def forward(self, embeddings, logits, labels):\n",
    "        \"\"\"\n",
    "        Computes the combined loss.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Embeddings (CLS token) produced by the model.\n",
    "            logits (torch.Tensor): Logits for binary classification.\n",
    "            labels (torch.Tensor): Ground truth labels for classification.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined loss (BCE loss + contrastive loss).\n",
    "        \"\"\"\n",
    "        global count1\n",
    "        # Compute BCE Loss\n",
    "        bce_loss = self.bce_loss(logits, labels.float().unsqueeze(1))\n",
    "        \n",
    "        # Compute Contrastive Loss\n",
    "        contrastive_loss = self.contrastive_loss(embeddings, labels)\n",
    "        \n",
    "        # Combine the Losses\n",
    "        combined_loss = bce_loss + self.lambda_contrastive * contrastive_loss\n",
    "        if count1 == 0:\n",
    "            print(bce_loss)\n",
    "            print(contrastive_loss)\n",
    "            count1 += 1\n",
    "        return combined_loss\n",
    "\n",
    "    def contrastive_loss(self, features, labels):\n",
    "        \"\"\"\n",
    "        Computes the contrastive loss between feature embeddings.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): Feature embeddings (output of ViT).\n",
    "            labels (torch.Tensor): Ground truth labels for contrastive loss computation.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Contrastive loss based on similarity between feature embeddings.\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        \n",
    "        # Create label masks\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask_pos = torch.eq(labels, labels.T).float()\n",
    "        \n",
    "        # Remove diagonal (self-similarities)\n",
    "        mask_pos.fill_diagonal_(0)\n",
    "        \n",
    "        # Compute positive and negative losses\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        eps = 1e-8  # Numerical stability\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + eps)\n",
    "        \n",
    "        # Mean over positive pairs\n",
    "        pos_mask_sum = mask_pos.sum(1).clamp(min=eps)\n",
    "        mean_log_prob_pos = (mask_pos * log_prob).sum(1) / pos_mask_sum\n",
    "        \n",
    "        return -mean_log_prob_pos.mean()\n",
    "\n",
    "\n",
    "# Initialize Loss and Optimizer - these values were chosen after several trials using Optuna\n",
    "temperature = 0.5\n",
    "lambda_contrastive = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "perturbed_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "train_dir = 'custom_dataset/train'\n",
    "test_dir = 'custom_dataset/test'\n",
    "perturbed_test_dir = 'test_dataset/test-interiit/perturbed_images_32'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit_with_combined_loss(defl_model, vit_model, train_dir, device):\n",
    "    \"\"\"\n",
    "    Trains the DEFL and ViT models together with a combined loss function.\n",
    "\n",
    "    This function trains the DEFL and ViT models on the provided training data \n",
    "    using a combined loss that includes Binary Cross-Entropy (BCE) and Contrastive Loss. \n",
    "    It computes feature representations using the DEFL model and combines them with \n",
    "    additional FFT and LBP features before passing them through the ViT model.\n",
    "\n",
    "    Args:\n",
    "        defl_model (nn.Module): The DEFL model used for feature extraction.\n",
    "        vit_model (nn.Module): The ViT model used for classification.\n",
    "        train_dir (str): Path to the training dataset.\n",
    "        device (torch.device): Device to run the models on (CPU or GPU).\n",
    "    \n",
    "    Saves:\n",
    "        The weights of both DEFL and ViT models to 'defl_weights.pth' and 'vit_weights.pth'.\n",
    "    \"\"\"\n",
    "    defl_model.train()  # Set DEFL model to train\n",
    "    vit_model.train()   # Set ViT model to train\n",
    "\n",
    "    # Load training data from the dataset directory\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    \n",
    "    optimizer = optim.Adam(list(defl_model.parameters()) + list(vit_model.parameters()), lr=0.001)  # Optimizer for both models\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Compute FFT and LBP features for the batch\n",
    "            fft_features = []\n",
    "            lbp_features = []\n",
    "            for img in images:  # Process each image in the batch\n",
    "                fft_features.append(compute_fft_magnitude(img))  # Now works with PyTorch tensors\n",
    "                lbp_features.append(compute_lbp(img))  # Now works with PyTorch tensors\n",
    "\n",
    "            # Convert FFT and LBP features to tensors\n",
    "            fft_features = torch.tensor(fft_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "            lbp_features = torch.tensor(lbp_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "\n",
    "            # Pass through DEFL\n",
    "            feature_maps = defl_model(images)  # Output: (B, 128, H, W)\n",
    "\n",
    "            # Concatenate DEFL, FFT, and LBP features\n",
    "            combined_features = torch.cat((feature_maps, fft_features, lbp_features), dim=1)  # (B, 130, H, W)\n",
    "\n",
    "            # Pass concatenated features through ViT\n",
    "            embeddings, logits = vit_model(combined_features)\n",
    "\n",
    "            # Compute combined loss (BCE + Contrastive Loss)\n",
    "            loss = combined_loss(embeddings, logits, labels)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate train accuracy\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Combined Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Save weights for both models\n",
    "    torch.save(defl_model.state_dict(), 'defl_weights.pth')\n",
    "    torch.save(vit_model.state_dict(), 'vit_weights.pth')\n",
    "\n",
    "def evaluate_vit_with_combined_loss(defl_model, vit_model, test_dir, device):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the trained DEFL and ViT models on the test dataset.\n",
    "\n",
    "    This function computes the accuracy, recall, precision, F1 score, and confusion matrix \n",
    "    by passing the test data through the DEFL and ViT models. It calculates the predictions \n",
    "    using the combined embeddings (from DEFL and ViT) and compares them to the true labels.\n",
    "\n",
    "    Args:\n",
    "        defl_model (nn.Module): The DEFL model used for feature extraction.\n",
    "        vit_model (nn.Module): The ViT model used for classification.\n",
    "        test_dir (str): Path to the test dataset.\n",
    "        device (torch.device): Device to run the models on (CPU or GPU).\n",
    "    \n",
    "    Prints:\n",
    "        Evaluation metrics including accuracy, recall, precision, F1 score, and confusion matrix.\n",
    "    \"\"\"\n",
    "    defl_model.eval()  # Set DEFL model to eval\n",
    "    vit_model.eval()   # Set ViT model to eval\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Compute FFT and LBP features for the batch\n",
    "            fft_features = []\n",
    "            lbp_features = []\n",
    "            for img in images:  # Process each image in the batch\n",
    "                fft_features.append(compute_fft_magnitude(img))  # Now works with PyTorch tensors\n",
    "                lbp_features.append(compute_lbp(img))  # Now works with PyTorch tensors\n",
    "\n",
    "            # Convert FFT and LBP features to tensors\n",
    "            fft_features = torch.tensor(fft_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "            lbp_features = torch.tensor(lbp_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "\n",
    "            # Pass through DEFL\n",
    "            feature_maps = defl_model(images)  # Output: (B, 128, H, W)\n",
    "\n",
    "            # Concatenate DEFL, FFT, and LBP features\n",
    "            combined_features = torch.cat((feature_maps, fft_features, lbp_features), dim=1)  # (B, 130, H, W)\n",
    "\n",
    "            # Pass concatenated features through ViT\n",
    "            embeddings, logits = vit_model(combined_features)\n",
    "\n",
    "            # Apply sigmoid activation and threshold\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Log evaluation results\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "def predict_and_save(vit_model, defl_model, test_dir, output_csv='predictions.csv', device='cuda'):\n",
    "    \"\"\"\n",
    "    Performs predictions on the test dataset and saves the results to a CSV file.\n",
    "\n",
    "    This function passes the test images through the DEFL and ViT models to generate predictions. \n",
    "    The results are then stored in a CSV file for further analysis.\n",
    "\n",
    "    Args:\n",
    "        vit_model (nn.Module): The ViT model used for classification.\n",
    "        defl_model (nn.Module): The DEFL model used for feature extraction.\n",
    "        test_dir (str): Path to the test dataset.\n",
    "        output_csv (str): Path to save the predictions (default: 'predictions.csv').\n",
    "        device (torch.device): Device to run the models on (CPU or GPU).\n",
    "    \n",
    "    Saves:\n",
    "        The predictions to the specified CSV file.\n",
    "    \"\"\"\n",
    "    vit_model.eval()\n",
    "    defl_model.eval()\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=perturbed_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Compute FFT and LBP features for the batch\n",
    "            fft_features = []\n",
    "            lbp_features = []\n",
    "            for img in images:  # Process each image in the batch\n",
    "                fft_features.append(compute_fft_magnitude(img))  # Now works with PyTorch tensors\n",
    "                lbp_features.append(compute_lbp(img))  # Now works with PyTorch tensors\n",
    "\n",
    "            # Convert FFT and LBP features to tensors\n",
    "            fft_features = torch.tensor(fft_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "            lbp_features = torch.tensor(lbp_features).unsqueeze(1).float().to(device)  # (B, 1, H, W)\n",
    "\n",
    "            # Pass through DEFL\n",
    "            feature_maps = defl_model(images)  # Output: (B, 128, H, W)\n",
    "\n",
    "            # Concatenate DEFL, FFT, and LBP features\n",
    "            combined_features = torch.cat((feature_maps, fft_features, lbp_features), dim=1)  # (B, 130, H, W)\n",
    "\n",
    "            # Pass concatenated features through ViT\n",
    "            _, logits = vit_model(combined_features)\n",
    "\n",
    "            # Apply sigmoid activation and threshold\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            # Store predictions\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df = pd.DataFrame(all_preds, columns=['Predictions'])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "# Initialize Loss and Optimizer - these values were chosen after several trials using Optuna\n",
    "vit_model = CombinedViTModel().to(device)\n",
    "defl_model = DEFL(composite_filters).to(device)\n",
    "temperature = 0.5\n",
    "lambda_contrastive = 0.01\n",
    "combined_loss = CombinedLoss(lambda_contrastive=lambda_contrastive).to(device)\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and save the model weights\n",
    "train_vit_with_combined_loss(defl_model, vit_model, train_dir='custom_dataset/train', device=device)\n",
    "\n",
    "# Load the saved weights for both models\n",
    "defl_model.load_state_dict(torch.load('defl_weights.pth'))\n",
    "vit_model.load_state_dict(torch.load('vit_weights.pth'))\n",
    "\n",
    "# Perform evaluation on test data\n",
    "evaluate_vit_with_combined_loss(defl_model, vit_model, test_dir='custom_dataset/test', device=device)\n",
    "\n",
    "# Perform predictions on new dataset and save results\n",
    "predict_and_save(vit_model, defl_model, test_dir='test_dataset/test-interiit/perturbed_images_32', output_csv='predictions.csv', device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Trainable Parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable Parameters in ViT: {count_parameters(vit_model)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6233914,
     "sourceId": 10106065,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6239027,
     "sourceId": 10112661,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
