{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# AuraSR: GAN-based Super-Resolution for real-world. Implementation is\n",
        "# based on the unofficial lucidrains/gigagan-pytorch repository. Heavily modified from there.\n",
        "#\n",
        "# https://mingukkang.github.io/GigaGAN/"
      ],
      "metadata": {
        "id": "3pzIotoRZ95Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "IOF7ni83Z9kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r Requirements_AuraSR_CIFAR10.txt"
      ],
      "metadata": {
        "id": "c3V5jQ5_aCKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports"
      ],
      "metadata": {
        "id": "qtAiLaUPaTnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2, ceil\n",
        "from functools import partial\n",
        "from typing import Any, Optional, List, Iterable\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch import nn, einsum, Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange, repeat, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "from torchvision.utils import save_image\n",
        "import math\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "jA9mWoHmacKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n"
      ],
      "metadata": {
        "id": "LxUrbiDXadUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_same_padding(size, kernel, dilation, stride):\n",
        "    return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n",
        "\n",
        "\n",
        "class AdaptiveConv2DMod(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    A 2D convolutional layer with adaptive kernels and optional demodulation.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        dim_out (int): Number of output channels.\n",
        "        kernel (int): Size of the convolution kernel (square).\n",
        "        demod (bool, optional): Whether to apply demodulation. Default is True.\n",
        "        stride (int, optional): Stride of the convolution. Default is 1.\n",
        "        dilation (int, optional): Dilation for convolution. Default is 1.\n",
        "        eps (float, optional): Small constant for stability. Default is 1e-8.\n",
        "        num_conv_kernels (int, optional): Number of convolution kernels. Default is 1 (no adaptation).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_out,\n",
        "        kernel,\n",
        "        *,\n",
        "        demod=True,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        eps=1e-8,\n",
        "        num_conv_kernels=1,  # set this to be greater than 1 for adaptive\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        self.dim_out = dim_out\n",
        "\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.adaptive = num_conv_kernels > 1\n",
        "\n",
        "        self.weights = nn.Parameter(\n",
        "            torch.randn((num_conv_kernels, dim_out, dim, kernel, kernel))\n",
        "        )\n",
        "\n",
        "        self.demod = demod\n",
        "\n",
        "        nn.init.kaiming_normal_(\n",
        "            self.weights, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\"\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, fmap, mod: Optional[Tensor] = None, kernel_mod: Optional[Tensor] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        notation\n",
        "\n",
        "        b - batch\n",
        "        n - convs\n",
        "        o - output\n",
        "        i - input\n",
        "        k - kernel\n",
        "        \"\"\"\n",
        "\n",
        "        b, h = fmap.shape[0], fmap.shape[-2]\n",
        "\n",
        "        # account for feature map that has been expanded by the scale in the first dimension\n",
        "        # due to multiscale inputs and outputs\n",
        "\n",
        "        if mod.shape[0] != b:\n",
        "            mod = repeat(mod, \"b ... -> (s b) ...\", s=b // mod.shape[0])\n",
        "\n",
        "        if exists(kernel_mod):\n",
        "            kernel_mod_has_el = kernel_mod.numel() > 0\n",
        "\n",
        "            assert self.adaptive or not kernel_mod_has_el\n",
        "\n",
        "            if kernel_mod_has_el and kernel_mod.shape[0] != b:\n",
        "                kernel_mod = repeat(\n",
        "                    kernel_mod, \"b ... -> (s b) ...\", s=b // kernel_mod.shape[0]\n",
        "                )\n",
        "\n",
        "        # prepare weights for modulation\n",
        "\n",
        "        weights = self.weights\n",
        "\n",
        "        if self.adaptive:\n",
        "            weights = repeat(weights, \"... -> b ...\", b=b)\n",
        "\n",
        "            # determine an adaptive weight and 'select' the kernel to use with softmax\n",
        "\n",
        "            assert exists(kernel_mod) and kernel_mod.numel() > 0\n",
        "\n",
        "            kernel_attn = kernel_mod.softmax(dim=-1)\n",
        "            kernel_attn = rearrange(kernel_attn, \"b n -> b n 1 1 1 1\")\n",
        "\n",
        "            weights = reduce(weights * kernel_attn, \"b n ... -> b ...\", \"sum\")\n",
        "\n",
        "        # do the modulation, demodulation, as done in stylegan2\n",
        "\n",
        "        mod = rearrange(mod, \"b i -> b 1 i 1 1\")\n",
        "\n",
        "        weights = weights * (mod + 1)\n",
        "\n",
        "        if self.demod:\n",
        "            inv_norm = (\n",
        "                reduce(weights**2, \"b o i k1 k2 -> b o 1 1 1\", \"sum\")\n",
        "                .clamp(min=self.eps)\n",
        "                .rsqrt()\n",
        "            )\n",
        "            weights = weights * inv_norm\n",
        "\n",
        "        fmap = rearrange(fmap, \"b c h w -> 1 (b c) h w\")\n",
        "\n",
        "        weights = rearrange(weights, \"b o ... -> (b o) ...\")\n",
        "\n",
        "        padding = get_same_padding(h, self.kernel, self.dilation, self.stride)\n",
        "        fmap = F.conv2d(fmap, weights, padding=padding, groups=b)\n",
        "\n",
        "        return rearrange(fmap, \"1 (b o) ... -> b o ...\", b=b)\n",
        "\n",
        "\n",
        "class Attend(nn.Module):\n",
        "    def __init__(self, dropout=0.0, flash=False):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.scale = nn.Parameter(torch.randn(1))\n",
        "        self.flash = flash\n",
        "\n",
        "    def flash_attn(self, q, k, v):\n",
        "        q, k, v = map(lambda t: t.contiguous(), (q, k, v))\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        if self.flash:\n",
        "            return self.flash_attn(q, k, v)\n",
        "\n",
        "        scale = q.shape[-1] ** -0.5\n",
        "\n",
        "        # similarity\n",
        "        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * scale\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # aggregate values\n",
        "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if callable(d) else d\n",
        "\n",
        "\n",
        "def cast_tuple(t, length=1):\n",
        "    if isinstance(t, tuple):\n",
        "        return t\n",
        "    return (t,) * length\n",
        "\n",
        "\n",
        "def identity(t, *args, **kwargs):\n",
        "    return t\n",
        "\n",
        "\n",
        "def is_power_of_two(n):\n",
        "    return log2(n).is_integer()\n",
        "\n",
        "\n",
        "def null_iterator():\n",
        "    while True:\n",
        "        yield None\n",
        "\n",
        "def Downsample(dim, dim_out=None):\n",
        "    return nn.Sequential(\n",
        "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
        "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
        "    )\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "        self.eps = 1e-4\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim=1) * self.g * (x.shape[1] ** 0.5)\n",
        "\n",
        "\n",
        "# building block modules\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8, num_conv_kernels=0):\n",
        "        super().__init__()\n",
        "        self.proj = AdaptiveConv2DMod(\n",
        "            dim, dim_out, kernel=3, num_conv_kernels=num_conv_kernels\n",
        "        )\n",
        "        self.kernel = 3\n",
        "        self.dilation = 1\n",
        "        self.stride = 1\n",
        "\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, conv_mods_iter: Optional[Iterable] = None):\n",
        "        conv_mods_iter = conv_mods_iter\n",
        "\n",
        "        x = self.proj(x, mod=next(conv_mods_iter), kernel_mod=next(conv_mods_iter))\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, dim, dim_out, *, groups=8, num_conv_kernels=0, style_dims: List = []\n",
        "    ):\n",
        "        super().__init__()\n",
        "        style_dims.extend([dim, num_conv_kernels, dim_out, num_conv_kernels])\n",
        "\n",
        "        self.block1 = Block(\n",
        "            dim, dim_out, groups=groups, num_conv_kernels=num_conv_kernels\n",
        "        )\n",
        "        self.block2 = Block(\n",
        "            dim_out, dim_out, groups=groups, num_conv_kernels=num_conv_kernels\n",
        "        )\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, conv_mods_iter: Optional[Iterable] = None):\n",
        "        h = self.block1(x, conv_mods_iter=conv_mods_iter)\n",
        "        h = self.block2(h, conv_mods_iter=conv_mods_iter)\n",
        "\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "\n",
        "        self.norm = RMSNorm(dim)\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), RMSNorm(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32, flash=False):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "\n",
        "        self.norm = RMSNorm(dim)\n",
        "\n",
        "        self.attend = Attend(flash=flash)\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h (x y) c\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        out = self.attend(q, k, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "# feedforward\n",
        "def FeedForward(dim, mult=4):\n",
        "    return nn.Sequential(\n",
        "        RMSNorm(dim),\n",
        "        nn.Conv2d(dim, dim * mult, 1),\n",
        "        nn.GELU(),\n",
        "        nn.Conv2d(dim * mult, dim, 1),\n",
        "    )\n",
        "\n",
        "\n",
        "# transformers\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, depth=1, flash_attn=True, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        Attention(\n",
        "                            dim=dim, dim_head=dim_head, heads=heads, flash=flash_attn\n",
        "                        ),\n",
        "                        FeedForward(dim=dim, mult=ff_mult),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearTransformer(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, depth=1, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        LinearAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                        FeedForward(dim=dim, mult=ff_mult),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NearestNeighborhoodUpsample(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None):\n",
        "        super().__init__()\n",
        "        dim_out = default(dim_out, dim)\n",
        "        self.conv = nn.Conv2d(dim, dim_out, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.shape[0] >= 64:\n",
        "            x = x.contiguous()\n",
        "\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, dim, dim_out, lr_mul=1, bias=True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(dim_out, dim))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(dim_out))\n",
        "\n",
        "        self.lr_mul = lr_mul\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
        "\n",
        "\n",
        "class StyleGanNetwork(nn.Module):\n",
        "    def __init__(self, dim_in=128, dim_out=512, depth=8, lr_mul=0.1, dim_text_latent=0):\n",
        "        super().__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.dim_text_latent = dim_text_latent\n",
        "\n",
        "        layers = []\n",
        "        for i in range(depth):\n",
        "            is_first = i == 0\n",
        "\n",
        "            if is_first:\n",
        "                dim_in_layer = dim_in + dim_text_latent\n",
        "            else:\n",
        "                dim_in_layer = dim_out\n",
        "\n",
        "            dim_out_layer = dim_out\n",
        "\n",
        "            layers.extend(\n",
        "                [EqualLinear(dim_in_layer, dim_out_layer, lr_mul), nn.LeakyReLU(0.2)]\n",
        "            )\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, text_latent=None):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        if self.dim_text_latent > 0:\n",
        "            assert exists(text_latent)\n",
        "            x = torch.cat((x, text_latent), dim=-1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UnetUpsampler(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        *,\n",
        "        image_size: int,\n",
        "        input_image_size: int,\n",
        "        init_dim: Optional[int] = None,\n",
        "        out_dim: Optional[int] = None,\n",
        "        style_network: Optional[dict] = None,\n",
        "        up_dim_mults: tuple = (1, 2, 4, 8, 16),\n",
        "        down_dim_mults: tuple = (4, 8, 16),\n",
        "        channels: int = 3,\n",
        "        resnet_block_groups: int = 8,\n",
        "        full_attn: tuple = (False, False, False, True, True),\n",
        "        flash_attn: bool = True,\n",
        "        self_attn_dim_head: int = 64,\n",
        "        self_attn_heads: int = 8,\n",
        "        attn_depths: tuple = (2, 2, 2, 2, 4),\n",
        "        mid_attn_depth: int = 4,\n",
        "        num_conv_kernels: int = 4,\n",
        "        resize_mode: str = \"bilinear\",\n",
        "        unconditional: bool = True,\n",
        "        skip_connect_scale: Optional[float] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.style_network = style_network = StyleGanNetwork(**style_network)\n",
        "        self.unconditional = unconditional\n",
        "        assert not (\n",
        "            unconditional\n",
        "            and exists(style_network)\n",
        "            and style_network.dim_text_latent > 0\n",
        "        )\n",
        "\n",
        "        assert is_power_of_two(image_size) and is_power_of_two(\n",
        "            input_image_size\n",
        "        ), \"both output image size and input image size must be power of 2\"\n",
        "        assert (\n",
        "            input_image_size < image_size\n",
        "        ), \"input image size must be smaller than the output image size, thus upsampling\"\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.input_image_size = input_image_size\n",
        "\n",
        "        style_embed_split_dims = []\n",
        "\n",
        "        self.channels = channels\n",
        "        input_channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim)\n",
        "\n",
        "        up_dims = [init_dim, *map(lambda m: dim * m, up_dim_mults)]\n",
        "        init_down_dim = up_dims[len(up_dim_mults) - len(down_dim_mults)]\n",
        "        down_dims = [init_down_dim, *map(lambda m: dim * m, down_dim_mults)]\n",
        "        self.init_conv = nn.Conv2d(input_channels, init_down_dim, 7, padding=3)\n",
        "\n",
        "        up_in_out = list(zip(up_dims[:-1], up_dims[1:]))\n",
        "        down_in_out = list(zip(down_dims[:-1], down_dims[1:]))\n",
        "\n",
        "        block_klass = partial(\n",
        "            ResnetBlock,\n",
        "            groups=resnet_block_groups,\n",
        "            num_conv_kernels=num_conv_kernels,\n",
        "            style_dims=style_embed_split_dims,\n",
        "        )\n",
        "\n",
        "        FullAttention = partial(Transformer, flash_attn=flash_attn)\n",
        "        *_, mid_dim = up_dims\n",
        "\n",
        "        self.skip_connect_scale = default(skip_connect_scale, 2**-0.5)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "\n",
        "        block_count = 6\n",
        "\n",
        "        for ind, (\n",
        "            (dim_in, dim_out),\n",
        "            layer_full_attn,\n",
        "            layer_attn_depth,\n",
        "        ) in enumerate(zip(down_in_out, full_attn, attn_depths)):\n",
        "            attn_klass = FullAttention if layer_full_attn else LinearTransformer\n",
        "\n",
        "            blocks = []\n",
        "            for i in range(block_count):\n",
        "                blocks.append(block_klass(dim_in, dim_in))\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        nn.ModuleList(blocks),\n",
        "                        nn.ModuleList(\n",
        "                            [\n",
        "                                (\n",
        "                                    attn_klass(\n",
        "                                        dim_in,\n",
        "                                        dim_head=self_attn_dim_head,\n",
        "                                        heads=self_attn_heads,\n",
        "                                        depth=layer_attn_depth,\n",
        "                                    )\n",
        "                                    if layer_full_attn\n",
        "                                    else None\n",
        "                                ),\n",
        "                                nn.Conv2d(\n",
        "                                    dim_in, dim_out, kernel_size=3, stride=2, padding=1\n",
        "                                ),\n",
        "                            ]\n",
        "                        ),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim)\n",
        "        self.mid_attn = FullAttention(\n",
        "            mid_dim,\n",
        "            dim_head=self_attn_dim_head,\n",
        "            heads=self_attn_heads,\n",
        "            depth=mid_attn_depth,\n",
        "        )\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim)\n",
        "\n",
        "        *_, last_dim = up_dims\n",
        "\n",
        "        for ind, (\n",
        "            (dim_in, dim_out),\n",
        "            layer_full_attn,\n",
        "            layer_attn_depth,\n",
        "        ) in enumerate(\n",
        "            zip(\n",
        "                reversed(up_in_out),\n",
        "                reversed(full_attn),\n",
        "                reversed(attn_depths),\n",
        "            )\n",
        "        ):\n",
        "            attn_klass = FullAttention if layer_full_attn else LinearTransformer\n",
        "\n",
        "            blocks = []\n",
        "            input_dim = dim_in * 2 if ind < len(down_in_out) else dim_in\n",
        "            for i in range(block_count):\n",
        "                blocks.append(block_klass(input_dim, dim_in))\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        nn.ModuleList(blocks),\n",
        "                        nn.ModuleList(\n",
        "                            [\n",
        "                                NearestNeighborhoodUpsample(\n",
        "                                    last_dim if ind == 0 else dim_out,\n",
        "                                    dim_in,\n",
        "                                ),\n",
        "                                (\n",
        "                                    attn_klass(\n",
        "                                        dim_in,\n",
        "                                        dim_head=self_attn_dim_head,\n",
        "                                        heads=self_attn_heads,\n",
        "                                        depth=layer_attn_depth,\n",
        "                                    )\n",
        "                                    if layer_full_attn\n",
        "                                    else None\n",
        "                                ),\n",
        "                            ]\n",
        "                        ),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.out_dim = default(out_dim, channels)\n",
        "        self.final_res_block = block_klass(dim, dim)\n",
        "        self.final_to_rgb = nn.Conv2d(dim, channels, 1)\n",
        "        self.resize_mode = resize_mode\n",
        "        self.style_to_conv_modulations = nn.Linear(\n",
        "            style_network.dim_out, sum(style_embed_split_dims)\n",
        "        )\n",
        "        self.style_embed_split_dims = style_embed_split_dims\n",
        "\n",
        "    @property\n",
        "    def allowable_rgb_resolutions(self):\n",
        "        input_res_base = int(log2(self.input_image_size))\n",
        "        output_res_base = int(log2(self.image_size))\n",
        "        allowed_rgb_res_base = list(range(input_res_base, output_res_base))\n",
        "        return [*map(lambda p: 2**p, allowed_rgb_res_base)]\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.style_network.net[0].weight.device\n",
        "\n",
        "    @property\n",
        "    def total_params(self):\n",
        "        return sum([p.numel() for p in self.parameters()])\n",
        "\n",
        "    def resize_image_to(self, x, size):\n",
        "        return F.interpolate(x, (size, size), mode=self.resize_mode)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        lowres_image: torch.Tensor,\n",
        "        styles: Optional[torch.Tensor] = None,\n",
        "        noise: Optional[torch.Tensor] = None,\n",
        "        global_text_tokens: Optional[torch.Tensor] = None,\n",
        "        return_all_rgbs: bool = False,\n",
        "    ):\n",
        "        x = lowres_image\n",
        "\n",
        "        noise_scale = 0.001  # Adjust the scale of the noise as needed\n",
        "        noise_aug = torch.randn_like(x) * noise_scale\n",
        "        x = x + noise_aug\n",
        "        x = x.clamp(0, 1)\n",
        "\n",
        "        shape = x.shape\n",
        "        batch_size = shape[0]\n",
        "\n",
        "        assert shape[-2:] == ((self.input_image_size,) * 2)\n",
        "\n",
        "        # styles\n",
        "        if not exists(styles):\n",
        "            assert exists(self.style_network)\n",
        "\n",
        "            noise = default(\n",
        "                noise,\n",
        "                torch.randn(\n",
        "                    (batch_size, self.style_network.dim_in), device= self.device\n",
        "                ),\n",
        "            )\n",
        "            styles = self.style_network(noise, global_text_tokens)\n",
        "\n",
        "        # project styles to conv modulations\n",
        "        conv_mods = self.style_to_conv_modulations(styles)\n",
        "        conv_mods = conv_mods.split(self.style_embed_split_dims, dim=-1)\n",
        "        conv_mods = iter(conv_mods)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        h = []\n",
        "        for blocks, (attn, downsample) in self.downs:\n",
        "            for block in blocks:\n",
        "                x = block(x, conv_mods_iter=conv_mods)\n",
        "                h.append(x)\n",
        "\n",
        "            if attn is not None:\n",
        "                x = attn(x)\n",
        "\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, conv_mods_iter=conv_mods)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, conv_mods_iter=conv_mods)\n",
        "\n",
        "        for (\n",
        "            blocks,\n",
        "            (\n",
        "                upsample,\n",
        "                attn,\n",
        "            ),\n",
        "        ) in self.ups:\n",
        "            x = upsample(x)\n",
        "            for block in blocks:\n",
        "                if h != []:\n",
        "                    res = h.pop()\n",
        "                    res = res * self.skip_connect_scale\n",
        "                    x = torch.cat((x, res), dim=1)\n",
        "\n",
        "                x = block(x, conv_mods_iter=conv_mods)\n",
        "\n",
        "            if attn is not None:\n",
        "                x = attn(x)\n",
        "\n",
        "        x = self.final_res_block(x, conv_mods_iter=conv_mods)\n",
        "        rgb = self.final_to_rgb(x)\n",
        "\n",
        "        if not return_all_rgbs:\n",
        "            return rgb\n",
        "\n",
        "        return rgb, []\n",
        "\n",
        "\n",
        "def tile_image(image, chunk_size=64):\n",
        "    c, h, w = image.shape\n",
        "    h_chunks = ceil(h / chunk_size)\n",
        "    w_chunks = ceil(w / chunk_size)\n",
        "    tiles = []\n",
        "    for i in range(h_chunks):\n",
        "        for j in range(w_chunks):\n",
        "            tile = image[:, i * chunk_size:(i + 1) * chunk_size, j * chunk_size:(j + 1) * chunk_size]\n",
        "            tiles.append(tile)\n",
        "    return tiles, h_chunks, w_chunks\n",
        "\n",
        "# This helps create a checkboard pattern with some edge blending\n",
        "def create_checkerboard_weights(tile_size):\n",
        "    x = torch.linspace(-1, 1, tile_size)\n",
        "    y = torch.linspace(-1, 1, tile_size)\n",
        "\n",
        "    x, y = torch.meshgrid(x, y, indexing='ij')\n",
        "    d = torch.sqrt(x*x + y*y)\n",
        "    sigma, mu = 0.5, 0.0\n",
        "    weights = torch.exp(-((d-mu)**2 / (2.0 * sigma**2)))\n",
        "\n",
        "    # saturate the values to sure get high weights in the center\n",
        "    weights = weights**8\n",
        "\n",
        "    return weights / weights.max()  # Normalize to [0, 1]\n",
        "\n",
        "def repeat_weights(weights, image_size):\n",
        "    tile_size = weights.shape[0]\n",
        "    repeats = (math.ceil(image_size[0] / tile_size), math.ceil(image_size[1] / tile_size))\n",
        "    return weights.repeat(repeats)[:image_size[0], :image_size[1]]\n",
        "\n",
        "def create_offset_weights(weights, image_size):\n",
        "    tile_size = weights.shape[0]\n",
        "    offset = tile_size // 2\n",
        "    full_weights = repeat_weights(weights, (image_size[0] + offset, image_size[1] + offset))\n",
        "    return full_weights[offset:, offset:]\n",
        "\n",
        "def merge_tiles(tiles, h_chunks, w_chunks, chunk_size=64):\n",
        "    # Determine the shape of the output tensor\n",
        "    c = tiles[0].shape[0]\n",
        "    h = h_chunks * chunk_size\n",
        "    w = w_chunks * chunk_size\n",
        "\n",
        "    # Create an empty tensor to hold the merged image\n",
        "    merged = torch.zeros((c, h, w), dtype=tiles[0].dtype)\n",
        "\n",
        "    # Iterate over the tiles and place them in the correct position\n",
        "    for idx, tile in enumerate(tiles):\n",
        "        i = idx // w_chunks\n",
        "        j = idx % w_chunks\n",
        "\n",
        "        h_start = i * chunk_size\n",
        "        w_start = j * chunk_size\n",
        "\n",
        "        tile_h, tile_w = tile.shape[1:]\n",
        "        merged[:, h_start:h_start+tile_h, w_start:w_start+tile_w] = tile\n",
        "\n",
        "    return merged\n",
        "\n",
        "class AuraSR:\n",
        "    def __init__(self, config: dict[str, Any], device: str = \"cuda\"):\n",
        "        self.upsampler = UnetUpsampler(**config).to(device)\n",
        "        self.input_image_size = config[\"input_image_size\"]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_id: str = \"fal-ai/AuraSR\", use_safetensors: bool = True, device: str = \"cuda\"):\n",
        "        import json\n",
        "        import torch\n",
        "        from pathlib import Path\n",
        "        from huggingface_hub import snapshot_download\n",
        "\n",
        "        # Check if model_id is a local file\n",
        "        if Path(model_id).is_file():\n",
        "            local_file = Path(model_id)\n",
        "            if local_file.suffix == '.safetensors':\n",
        "                use_safetensors = True\n",
        "            elif local_file.suffix == '.ckpt':\n",
        "                use_safetensors = False\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file format: {local_file.suffix}. Please use .safetensors or .ckpt files.\")\n",
        "\n",
        "            # For local files, we need to provide the config separately\n",
        "            config_path = local_file.with_name('config.json')\n",
        "            if not config_path.exists():\n",
        "                raise FileNotFoundError(\n",
        "                    f\"Config file not found: {config_path}. \"\n",
        "                    f\"When loading from a local file, ensure that 'config.json' \"\n",
        "                    f\"is present in the same directory as '{local_file.name}'. \"\n",
        "                    f\"If you're trying to load a model from Hugging Face, \"\n",
        "                    f\"please provide the model ID instead of a file path.\"\n",
        "                )\n",
        "\n",
        "            config = json.loads(config_path.read_text())\n",
        "            hf_model_path = local_file.parent\n",
        "        else:\n",
        "            hf_model_path = Path(snapshot_download(model_id))\n",
        "            config = json.loads((hf_model_path / \"config.json\").read_text())\n",
        "\n",
        "        model = cls(config, device)\n",
        "\n",
        "        if use_safetensors:\n",
        "            try:\n",
        "                from safetensors.torch import load_file\n",
        "                checkpoint = load_file(hf_model_path / \"model.safetensors\" if not Path(model_id).is_file() else model_id)\n",
        "            except ImportError:\n",
        "                raise ImportError(\n",
        "                    \"The safetensors library is not installed. \"\n",
        "                    \"Please install it with `pip install safetensors` \"\n",
        "                    \"or use `use_safetensors=False` to load the model with PyTorch.\"\n",
        "                )\n",
        "        else:\n",
        "            checkpoint = torch.load(hf_model_path / \"model.ckpt\" if not Path(model_id).is_file() else model_id)\n",
        "\n",
        "        model.upsampler.load_state_dict(checkpoint, strict=True)\n",
        "        return model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def upscale_4x(self, image: Image.Image, max_batch_size=8) -> Image.Image:\n",
        "        tensor_transform = transforms.ToTensor()\n",
        "        device = self.upsampler.device\n",
        "\n",
        "        image_tensor = tensor_transform(image).unsqueeze(0)\n",
        "        _, _, h, w = image_tensor.shape\n",
        "        pad_h = (self.input_image_size - h % self.input_image_size) % self.input_image_size\n",
        "        pad_w = (self.input_image_size - w % self.input_image_size) % self.input_image_size\n",
        "\n",
        "        # Pad the image\n",
        "        image_tensor = torch.nn.functional.pad(image_tensor, (0, pad_w, 0, pad_h), mode='reflect').squeeze(0)\n",
        "        tiles, h_chunks, w_chunks = tile_image(image_tensor, self.input_image_size)\n",
        "\n",
        "        # Batch processing of tiles\n",
        "        num_tiles = len(tiles)\n",
        "        batches = [tiles[i:i + max_batch_size] for i in range(0, num_tiles, max_batch_size)]\n",
        "        reconstructed_tiles = []\n",
        "\n",
        "        for batch in batches:\n",
        "            model_input = torch.stack(batch).to(device)\n",
        "            generator_output = self.upsampler(\n",
        "                lowres_image=model_input,\n",
        "                noise=torch.randn(model_input.shape[0], 128, device=device)\n",
        "            )\n",
        "            reconstructed_tiles.extend(list(generator_output.clamp_(0, 1).detach().cpu()))\n",
        "\n",
        "        merged_tensor = merge_tiles(reconstructed_tiles, h_chunks, w_chunks, self.input_image_size * 4)\n",
        "        unpadded = merged_tensor[:, :h * 4, :w * 4]\n",
        "\n",
        "        to_pil = transforms.ToPILImage()\n",
        "        return to_pil(unpadded)\n",
        "    # Tiled 4x upscaling with overlapping tiles to reduce seam artifacts\n",
        "    # weights options are 'checkboard' and 'constant'\n",
        "    @torch.no_grad()\n",
        "    def upscale_4x_overlapped(self, image, max_batch_size=16, weight_type='checkboard'):\n",
        "        tensor_transform = transforms.ToTensor()\n",
        "        device = self.upsampler.device\n",
        "\n",
        "        image_tensor = tensor_transform(image).unsqueeze(0)\n",
        "        _, _, h, w = image_tensor.shape\n",
        "\n",
        "        # Calculate paddings\n",
        "        pad_h = (\n",
        "            self.input_image_size - h % self.input_image_size\n",
        "        ) % self.input_image_size\n",
        "        pad_w = (\n",
        "            self.input_image_size - w % self.input_image_size\n",
        "        ) % self.input_image_size\n",
        "\n",
        "        pad_h = min(pad_h-1, self.input_image_size-1)\n",
        "        pad_w = min(pad_w-1, self.input_image_size-1)\n",
        "        # Pad the image\n",
        "        image_tensor = torch.nn.functional.pad(\n",
        "            image_tensor, (0, pad_w, 0, pad_h), mode=\"reflect\"\n",
        "        ).squeeze(0)\n",
        "\n",
        "        # Function to process tiles\n",
        "        def process_tiles(tiles, h_chunks, w_chunks):\n",
        "            num_tiles = len(tiles)\n",
        "            batches = [\n",
        "                tiles[i : i + max_batch_size]\n",
        "                for i in range(0, num_tiles, max_batch_size)\n",
        "            ]\n",
        "            reconstructed_tiles = []\n",
        "\n",
        "            for batch in batches:\n",
        "                model_input = torch.stack(batch).to(device)\n",
        "                generator_output = self.upsampler(\n",
        "                    lowres_image=model_input,\n",
        "                    noise=torch.randn(model_input.shape[0], 128, device=device),\n",
        "                )\n",
        "                reconstructed_tiles.extend(\n",
        "                    list(generator_output.clamp_(0, 1).detach().cpu())\n",
        "                )\n",
        "\n",
        "            return merge_tiles(\n",
        "                reconstructed_tiles, h_chunks, w_chunks, self.input_image_size\n",
        "            )\n",
        "\n",
        "        # First pass\n",
        "        tiles1, h_chunks1, w_chunks1 = tile_image(image_tensor, self.input_image_size)\n",
        "        result1 = process_tiles(tiles1, h_chunks1, w_chunks1)\n",
        "\n",
        "        # Second pass with offset\n",
        "        offset = self.input_image_size // 2\n",
        "        image_tensor_offset = torch.nn.functional.pad(image_tensor, (offset, offset, offset, offset), mode='reflect').squeeze(0)\n",
        "\n",
        "        tiles2, h_chunks2, w_chunks2 = tile_image(\n",
        "            image_tensor_offset, self.input_image_size\n",
        "        )\n",
        "        result2 = process_tiles(tiles2, h_chunks2, w_chunks2)\n",
        "\n",
        "        # unpad\n",
        "        offset_4x = offset * 1\n",
        "        result2_interior = result2[:, offset_4x:-offset_4x, offset_4x:-offset_4x]\n",
        "\n",
        "        if weight_type == 'checkboard':\n",
        "            weight_tile = create_checkerboard_weights(self.input_image_size * 1)\n",
        "\n",
        "            weight_shape = result2_interior.shape[1:]\n",
        "            weights_1 = create_offset_weights(weight_tile, weight_shape)\n",
        "            weights_2 = repeat_weights(weight_tile, weight_shape)\n",
        "\n",
        "            normalizer = weights_1 + weights_2\n",
        "            weights_1 = weights_1 / normalizer\n",
        "            weights_2 = weights_2 / normalizer\n",
        "\n",
        "            weights_1 = weights_1.unsqueeze(0).repeat(3, 1, 1)\n",
        "            weights_2 = weights_2.unsqueeze(0).repeat(3, 1, 1)\n",
        "        elif weight_type == 'constant':\n",
        "            weights_1 = torch.ones_like(result2_interior) * 0.5\n",
        "            weights_2 = weights_1\n",
        "        else:\n",
        "            raise ValueError(\"weight_type should be either 'gaussian' or 'constant' but got\", weight_type)\n",
        "\n",
        "        result1 = result1 * weights_2\n",
        "        result2 = result2_interior * weights_1\n",
        "\n",
        "        # Average the overlapping region\n",
        "        result1 = (\n",
        "            result1 + result2\n",
        "        )\n",
        "\n",
        "        # Remove padding\n",
        "        unpadded = result1[:, : h, : w]\n",
        "\n",
        "        to_pil = transforms.ToPILImage()\n",
        "        return to_pil(unpadded)"
      ],
      "metadata": {
        "id": "l1Z1wXDzTb4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "m-asNw_0boBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aura_sr = AuraSR.from_pretrained()"
      ],
      "metadata": {
        "id": "ffxd_RpUS0or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    image_data = BytesIO(response.content)\n",
        "    return Image.open(image_data)"
      ],
      "metadata": {
        "id": "VW7XXU4HUgXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './'\n",
        "cifar10 =  CIFAR10(data_path, train=True, download=True)\n",
        "len(cifar10)"
      ],
      "metadata": {
        "id": "E3YTl-OWIsJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(cifar10)):\n",
        "\n",
        "    img, label = cifar10[i]\n",
        "    stacked_image = Image.new('RGB', (64, 64))\n",
        "\n",
        "    # Paste the image 4 times to stack\n",
        "    stacked_image.paste(img, (0, 0))         # Top-left\n",
        "    stacked_image.paste(img, (32, 0))       # Top-right\n",
        "    stacked_image.paste(img, (0, 32))       # Bottom-left\n",
        "    stacked_image.paste(img, (32, 32))\n",
        "    upscaled_image = aura_sr.upscale_4x(stacked_image)\n",
        "    upscaled_image\n",
        "    width, height = 256,256\n",
        "\n",
        "    # Define the crop box for the top-left quarter\n",
        "    crop_box = (0, 0, width // 2, height // 2)\n",
        "\n",
        "    # Crop the image\n",
        "    cropped_img = upscaled_image.crop(crop_box)\n",
        "    cropped_img.resize((32,32)).save(\"/content/Aura_SR_train/{}.jpg\".format(i))\n",
        "    if i%100 == 0:\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "CmuUKXKdLXCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/Aura_SR_train.zip /content/Aura_SR_train"
      ],
      "metadata": {
        "id": "bzLLIfVF5bGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-B14lxx90U_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}