{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 5256696,
          "sourceType": "datasetVersion",
          "datasetId": 3041726
        },
        {
          "sourceId": 10108304,
          "sourceType": "datasetVersion",
          "datasetId": 6235702
        },
        {
          "sourceId": 10108337,
          "sourceType": "datasetVersion",
          "datasetId": 6235725
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## installing dependencies"
      ],
      "metadata": {
        "id": "yj2XRT49XGJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchjd"
      ],
      "metadata": {
        "id": "TGSldMeIa36w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports"
      ],
      "metadata": {
        "id": "H57rRA1ucG5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import models as M\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch.nn import MSELoss, KLDivLoss, CrossEntropyLoss, L1Loss\n",
        "from torch.optim import Adam\n",
        "from torchjd import mtl_backward\n",
        "from torchjd.aggregation import UPGrad\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WtxOFF-d9h_3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:11.322719Z",
          "iopub.execute_input": "2024-12-05T10:23:11.322997Z",
          "iopub.status.idle": "2024-12-05T10:23:11.332300Z",
          "shell.execute_reply.started": "2024-12-05T10:23:11.322971Z",
          "shell.execute_reply": "2024-12-05T10:23:11.331538Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## specify device"
      ],
      "metadata": {
        "id": "nAb40zwhXuRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:11.951267Z",
          "iopub.execute_input": "2024-12-05T10:23:11.951676Z",
          "iopub.status.idle": "2024-12-05T10:23:12.040029Z",
          "shell.execute_reply.started": "2024-12-05T10:23:11.951647Z",
          "shell.execute_reply": "2024-12-05T10:23:12.039121Z"
        },
        "id": "_VIzfcBlWChV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformations"
      ],
      "metadata": {
        "id": "AnesX-2TXzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),\n",
        "    ], p=0.3)\n",
        "])"
      ],
      "metadata": {
        "id": "586-PwxuREkz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:24:35.605758Z",
          "iopub.execute_input": "2024-12-05T10:24:35.606080Z",
          "iopub.status.idle": "2024-12-05T10:25:48.360390Z",
          "shell.execute_reply.started": "2024-12-05T10:24:35.606055Z",
          "shell.execute_reply": "2024-12-05T10:25:48.359694Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## path to training and testing dataset, creation of val datasets and data loaders"
      ],
      "metadata": {
        "id": "74aXL_msZTJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage with your data loaders\n",
        "train_data = datasets.ImageFolder(root=\"path to folder containing only unperturbed images - training dataset for teacher\", transform=transform)\n",
        "\n",
        "# Train-Val Split\n",
        "train_size = int(0.8 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_data, val_data = random_split(train_data, [train_size, val_size])\n",
        "\n",
        "test_data = datasets.ImageFolder(root=\"path to folder containing both perturbed and unperturbed images - test dataset for student\", transform=transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "7KV5UhENaKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## common architecture for student and teacher"
      ],
      "metadata": {
        "id": "Bqp1La42aVG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        # Modify the fully connected layer to output 2 classes\n",
        "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, 2)\n",
        "\n",
        "        # Modify the first convolutional layer to output 16 channels, as expected by BasicBlock\n",
        "        self.encoder.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.encoder.bn1 = nn.BatchNorm2d(16)  # This should match the output channels of the first conv layer\n",
        "        self.encoder.maxpool = nn.Identity()  # Avoid max pooling for now\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-processing\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)  # Match the number of channels after conv1\n",
        "        x = self.encoder.relu(x)\n",
        "        x = self.encoder.maxpool(x)\n",
        "\n",
        "        # Pass through ResNet20 layers\n",
        "        x1 = self.encoder.layer1(x)  # First block\n",
        "        x2 = self.encoder.layer2(x1)  # Second block\n",
        "        x3 = self.encoder.layer3(x2)  # Third block\n",
        "\n",
        "        # Global Average Pooling and fully connected layers\n",
        "        x = self.encoder.avgpool(x3)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.encoder.fc(x)\n",
        "\n",
        "        return x, [x1, x2, x3]\n"
      ],
      "metadata": {
        "id": "OovUQesr7rnE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.367755Z",
          "iopub.execute_input": "2024-12-05T10:25:48.368009Z",
          "iopub.status.idle": "2024-12-05T10:25:48.379767Z",
          "shell.execute_reply.started": "2024-12-05T10:25:48.367985Z",
          "shell.execute_reply": "2024-12-05T10:25:48.379049Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection for downsampling\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "oprZBOtHCk_B",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.396069Z",
          "iopub.execute_input": "2024-12-05T10:25:48.396272Z",
          "iopub.status.idle": "2024-12-05T10:25:48.409910Z",
          "shell.execute_reply.started": "2024-12-05T10:25:48.396252Z",
          "shell.execute_reply": "2024-12-05T10:25:48.409118Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        # Initial convolutional block\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Three residual layers\n",
        "        self.layer1 = self._make_layer(16, num_blocks=3, stride=1)  # No downsampling\n",
        "        self.layer2 = self._make_layer(32, num_blocks=3, stride=2)  # Downsampling\n",
        "        self.layer3 = self._make_layer(64, num_blocks=3, stride=2)  # Downsampling\n",
        "\n",
        "        # Global average pooling and fully connected layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Create a residual block with the specified number of blocks and stride.\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1)  # First block may downsample\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels  # Update in_channels for the next block\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial convolutional block\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Residual layers\n",
        "        x1 = self.layer1(x)  # First residual layer\n",
        "        x2 = self.layer2(x1)  # Second residual layer\n",
        "        x3 = self.layer3(x2)  # Third residual layer\n",
        "\n",
        "        # Global average pooling and fully connected layer\n",
        "        x = self.avgpool(x3)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x, [x1, x2, x3]\n"
      ],
      "metadata": {
        "id": "fNfZsMQE7_Qk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.410726Z",
          "iopub.execute_input": "2024-12-05T10:25:48.410929Z",
          "iopub.status.idle": "2024-12-05T10:25:48.429361Z",
          "shell.execute_reply.started": "2024-12-05T10:25:48.410908Z",
          "shell.execute_reply": "2024-12-05T10:25:48.428522Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## teacher model"
      ],
      "metadata": {
        "id": "JqfqpCh0arXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ResNet20(num_classes=2)\n",
        "\n",
        "teacher = Model(encoder)\n",
        "teacher = teacher.to(device)\n",
        "teacher.train()\n",
        "None"
      ],
      "metadata": {
        "id": "ATzMBQloREk2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.442605Z",
          "iopub.execute_input": "2024-12-05T10:25:48.442853Z",
          "iopub.status.idle": "2024-12-05T10:25:48.674712Z",
          "shell.execute_reply.started": "2024-12-05T10:25:48.442829Z",
          "shell.execute_reply": "2024-12-05T10:25:48.673769Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training function - teacher"
      ],
      "metadata": {
        "id": "wypeVWyNafIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the teacher\n",
        "def train_model_teacher(model, train_dataloader, val_dataloader, epochs, lr, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train the teacher model with early stopping on the validation dataset.\n",
        "    Args:\n",
        "        model (nn.Module): The teacher model to train.\n",
        "        train_dataloader (DataLoader): Dataloader for the training set.\n",
        "        val_dataloader (DataLoader): Dataloader for the validation set.\n",
        "        epochs (int): Number of epochs to train.\n",
        "        lr (float): Learning rate.\n",
        "    Returns:\n",
        "        nn.Module: The trained model with the best weights loaded.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 4\n",
        "    counter = 0\n",
        "    delta = 0\n",
        "    best_model_path = './best_teacher_model.pth'\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for i, data in enumerate(tqdm(train_dataloader), 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_avg_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1} Train Loss: {train_avg_loss}\")\n",
        "\n",
        "        # Evaluate on Train and Validation datasets\n",
        "        train_acc, train_report, _ = evaluate(model, train_dataloader, device)\n",
        "        val_acc, val_report, val_avg_loss = evaluate(model, val_dataloader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} Train Accuracy: {train_acc:.4f}\")\n",
        "        print(train_report)\n",
        "        print(f\"Epoch {epoch + 1} Validation Loss: {val_avg_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
        "        print(val_report)\n",
        "\n",
        "        # Early stopping on validation loss\n",
        "        if val_avg_loss < best_val_loss - delta:\n",
        "            best_val_loss = val_avg_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved at epoch {epoch + 1}.\")\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"No improvement. Early stopping patience counter: {counter}/{patience}\")\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Load the best model weights before returning\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(\"Loaded the best model weights.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "w8aKbUO7REk2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.676191Z",
          "iopub.execute_input": "2024-12-05T10:25:48.676582Z",
          "iopub.status.idle": "2024-12-05T10:25:48.689091Z",
          "shell.execute_reply.started": "2024-12-05T10:25:48.676543Z",
          "shell.execute_reply": "2024-12-05T10:25:48.688240Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation function"
      ],
      "metadata": {
        "id": "GM_hQXprcs64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate(model, dataloader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataloader.\n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        dataloader (DataLoader): The dataloader for evaluation.\n",
        "        device (str): The device to use for computation.\n",
        "    Returns:\n",
        "        float: Accuracy of the model on the dataset.\n",
        "        dict: Classification report as a dictionary.\n",
        "        float: Average loss on the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    running_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(dataloader):\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs, _ = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    return correct / total, classification_report(all_labels, all_preds, output_dict=True), avg_loss"
      ],
      "metadata": {
        "id": "8j83z3PocpDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training of teacher"
      ],
      "metadata": {
        "id": "BH1t65B_c1tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_teacher(teacher,train_dataloader,test_dataloader,25,0.0001)"
      ],
      "metadata": {
        "id": "Odc1yG9SREk3",
        "outputId": "19bdea68-87fe-490e-b7eb-0bb6d75b4cdf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:25:48.690860Z",
          "iopub.execute_input": "2024-12-05T10:25:48.691106Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": " 72%|███████▏  | 969/1345 [03:45<01:20,  4.70it/s]",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## student model"
      ],
      "metadata": {
        "id": "11YPc3_raxg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ResNet20(num_classes=2)\n",
        "\n",
        "student = Model(encoder)\n",
        "student = student.to(device)"
      ],
      "metadata": {
        "id": "Fwufpcq4REk4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:12.343204Z",
          "iopub.status.idle": "2024-12-05T10:23:12.343643Z",
          "shell.execute_reply.started": "2024-12-05T10:23:12.343401Z",
          "shell.execute_reply": "2024-12-05T10:23:12.343424Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adversarial attacks - FGSM and PGD"
      ],
      "metadata": {
        "id": "iuONsXSobDip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, x, y, eps=0.03, targeted=False):\n",
        "    model.eval()\n",
        "    x = x.to(device)  # Ensure x is on the same device as the model\n",
        "    y = y.to(device)  # Ensure y is on the same device as the model\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "\n",
        "    output,_ = model(x_adv)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if targeted:\n",
        "            perturb = eps * torch.sign(-x_adv.grad)\n",
        "        else:\n",
        "            perturb = eps * torch.sign(x_adv.grad)\n",
        "\n",
        "        x_adv = torch.clamp(x_adv + perturb, min=0, max=1)\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "# PGD attack function\n",
        "def pgd_attack(model, x, y, eps=0.03, alpha=0.01, steps=2, targeted=False):\n",
        "    model.eval()\n",
        "    x = x.to(device)  # Ensure x is on the same device as the model\n",
        "    y = y.to(device)  # Ensure y is on the same device as the model\n",
        "    x_adv = x.clone().detach()\n",
        "\n",
        "    x_adv = x_adv + torch.empty_like(x_adv).uniform_(-eps, eps)\n",
        "    x_adv = torch.clamp(x_adv, min=0, max=1)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(steps):\n",
        "        x_adv.requires_grad_(True)\n",
        "\n",
        "        output,_ = model(x_adv)\n",
        "        loss = criterion(output, y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if targeted:\n",
        "                perturb = -alpha * torch.sign(x_adv.grad)\n",
        "            else:\n",
        "                perturb = alpha * torch.sign(x_adv.grad)\n",
        "\n",
        "            x_adv += perturb\n",
        "            x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)\n",
        "            x_adv = torch.clamp(x_adv, min=0, max=1)\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "import random\n",
        "def attackit(x,y, model,p_orig=0.2,p_pgd=0.4,p_fgsm=0.4):\n",
        "    rn = random.uniform(0,1)\n",
        "    if rn < p_orig:\n",
        "        return x,y\n",
        "    elif rn < p_orig + p_pgd:\n",
        "        return pgd_attack(model,x,y),y\n",
        "    else:\n",
        "        return fgsm_attack(model,x,y),y"
      ],
      "metadata": {
        "id": "Ud6x-oQEbEBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training and evaluation function - student"
      ],
      "metadata": {
        "id": "-Ylos5UXbMbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_student_jd(student, teacher, train_dataloader, test_dataloader, aug_fn, epochs, lr, device='cuda'):\n",
        "    \"\"\"\n",
        "    Train a student model using knowledge distillation with Jacobian Descent for optimization.\n",
        "\n",
        "    Args:\n",
        "        student (nn.Module): The student model to be trained.\n",
        "        teacher (nn.Module): The teacher model providing supervision.\n",
        "        train_dataloader (DataLoader): Dataloader for the training set.\n",
        "        test_dataloader (DataLoader): Dataloader for the testing set.\n",
        "        aug_fn (function): Data augmentation function.\n",
        "        epochs (int): Number of epochs to train.\n",
        "        lr (float): Learning rate.\n",
        "        device (str): Device to run the training on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained student model.\n",
        "    \"\"\"\n",
        "    eps = 1e-4\n",
        "    optimizer = Adam(student.parameters(), lr=lr)\n",
        "    criterion = CrossEntropyLoss()  # Supervised loss for classification\n",
        "    # latent_loss_fn = MSELoss()  # Loss for intermediate feature matching\n",
        "    latent_loss_fn = L1Loss()  # Loss for intermediate feature matching\n",
        "    kl_div_fn = KLDivLoss(reduction=\"batchmean\")  # KL divergence for logits matching\n",
        "    aggregation_strategy = UPGrad()  # Strategy for aggregating gradients\n",
        "\n",
        "    teacher.eval()  # Teacher in evaluation mode\n",
        "    student.to(device)\n",
        "    teacher.to(device)\n",
        "\n",
        "    # Variables to track best loss and save best model\n",
        "    best_loss = float('inf')\n",
        "    best_model_path = './best_student_model.pth'\n",
        "    import os\n",
        "    # Create the parent directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        student.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(tqdm(train_dataloader), 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Generate teacher outputs\n",
        "            with torch.no_grad():\n",
        "                t_logits, [t1, t2, t3] = teacher(inputs)\n",
        "\n",
        "            # Apply data augmentation (basically - adversarial attack)\n",
        "            inputs, labels = aug_fn(inputs, labels)\n",
        "\n",
        "            # Forward pass for the student\n",
        "            s_logits, [s1, s2, s3] = student(inputs)\n",
        "\n",
        "            # Compute individual losses\n",
        "            classification_loss = criterion(s_logits, labels)\n",
        "            intermediate_loss = (\n",
        "                latent_loss_fn(s1, t1) +\n",
        "                latent_loss_fn(s2, t2) +\n",
        "                latent_loss_fn(s3, t3)\n",
        "                # latent_loss_fn(s4, t4)\n",
        "            )\n",
        "            temperature = 3.0\n",
        "            kl_loss = kl_div_fn(\n",
        "                torch.log_softmax(s_logits / temperature, dim=1),\n",
        "                torch.softmax(t_logits / temperature, dim=1)\n",
        "            ) * (temperature ** 2)\n",
        "\n",
        "            # Define the losses as separate tasks\n",
        "            losses = [classification_loss, intermediate_loss, kl_loss]\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Apply multi-task learning backward\n",
        "            mtl_backward(\n",
        "                losses=losses,\n",
        "                features=s_logits,  # Feature space is shared logits\n",
        "                tasks_params=[[]]*len(losses),  # Student parameters per task\n",
        "                shared_params=student.parameters(),  # Shared parameters\n",
        "                A=aggregation_strategy\n",
        "            )\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for logging\n",
        "            running_loss += sum(loss.item() for loss in losses)\n",
        "\n",
        "        # Logging after each epoch\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | Classification Loss: {classification_loss:.4f} | Intermediate Loss: {intermediate_loss:.4f} | KL Loss: {kl_loss:.4f}\")\n",
        "\n",
        "        # Save the best model if current loss is lower\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(student.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved with loss {best_loss:.4f} at epoch {epoch + 1}.\")\n",
        "\n",
        "        # Evaluate model\n",
        "        acc, report = evaluate(student, test_dataloader)\n",
        "        print(f\"Epoch {epoch + 1} | Accuracy: {acc:.2f}\")\n",
        "        print(report)\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "0Y_Pur0bDvIm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:12.353185Z",
          "iopub.status.idle": "2024-12-05T10:23:12.354012Z",
          "shell.execute_reply.started": "2024-12-05T10:23:12.353778Z",
          "shell.execute_reply": "2024-12-05T10:23:12.353802Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training of student"
      ],
      "metadata": {
        "id": "0N_TT3ozdSx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher.eval()\n",
        "aug_fn = lambda x,y: attackit(x,y, teacher,0.2,0.4,0.4)\n",
        "train_model_student_jd(student,teacher,train_dataloader,test_dataloader,aug_fn,3,0.0001)"
      ],
      "metadata": {
        "id": "B04aSKXcREk5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:12.355236Z",
          "iopub.status.idle": "2024-12-05T10:23:12.356124Z",
          "shell.execute_reply.started": "2024-12-05T10:23:12.355886Z",
          "shell.execute_reply": "2024-12-05T10:23:12.355910Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "j3-tTGKvREk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(encoder)\n",
        "model.load_state_dict(torch.load('./best_student_model.pth'))"
      ],
      "metadata": {
        "id": "AvPKmXqlZ0Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "model.to(device)\n",
        "for data in test_dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs, _ = model(inputs)\n",
        "    probs = F.softmax(outputs, dim=1)\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "    preds = torch.argmax(probs, dim=1)\n",
        "    all_preds.extend(preds.cpu().numpy())"
      ],
      "metadata": {
        "id": "dZ1V2D3QREk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:12.362249Z",
          "iopub.status.idle": "2024-12-05T10:23:12.362586Z",
          "shell.execute_reply.started": "2024-12-05T10:23:12.362408Z",
          "shell.execute_reply": "2024-12-05T10:23:12.362423Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store all predictions and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Assuming your model is already on the device\n",
        "model.to(device)\n",
        "\n",
        "# Loop through the test dataloader\n",
        "for data in test_dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Get model outputs\n",
        "    outputs, _ = model(inputs)\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "    # Convert labels and predictions to numpy arrays for evaluation\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "    preds = torch.argmax(probs, dim=1)\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "7D1xhDnhjO7o",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-05T10:23:12.364031Z",
          "iopub.status.idle": "2024-12-05T10:23:12.364299Z",
          "shell.execute_reply.started": "2024-12-05T10:23:12.364167Z",
          "shell.execute_reply": "2024-12-05T10:23:12.364180Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}